library("sparklyr")
library("dplyr")
library("purrr")
library("rsparkling")
x = spark_read_csv(sc, "x")
y = spark_read_csv(sc, "y")
sc <- spark_connect(master = "local", config = conf)
# >>>>> CONFIG SPARK <<<<<
spark_install()
conf <- spark_config()
conf$sparklyr.cores.local <- 2
conf$'sparklyr.shell.driver-memory' <- "8G"
conf$spark.memory.fraction <- 0.9
# >>>>> CONNECT TO NODE <<<<<
sc <- spark_connect(master = "local", config = conf)
sc <- spark_connect(master = "local", config = conf)
# >>>>> Load CSV INTO SPARK <<<<<
x = spark_read_csv(sc, "x")
y = spark_read_csv(sc, "y")
sc <- spark_connect(master = "local", config = conf)
detach()
detach(rsparkling)
detach("rsparkling")
library("sparklyr")
library("dplyr")
library("purrr")
spark_install()
conf <- spark_config()
conf$sparklyr.cores.local <- 2
conf$'sparklyr.shell.driver-memory' <- "8G"
conf$spark.memory.fraction <- 0.9
# >>>>> CONNECT TO NODE <<<<<
sc <- spark_connect(master = "local", config = conf)
# >>>>> Load CSV INTO SPARK <<<<<
x = spark_read_csv(sc, "x")
y = spark_read_csv(sc, "y")
# >>>>> Load CSV INTO SPARK <<<<<
x = spark_read_csv(sc, path = "x")
y = spark_read_csv(sc, path = "y")
# >>>>> Load CSV INTO SPARK <<<<<
x = spark_read_csv(sc, name = "x", path = "x")
y = spark_read_csv(sc, name = "y", path = "y")
setwd("~/Dropbox/KSULasso/R/SparkSandbox/")
x = spark_read_csv(sc, name = "x", path = "x")
y = spark_read_csv(sc, name = "y", path = "y")
# Converts R dataframe into spark dataframe.
all_data <- copy_to(sc, all_data_R, overwrite = TRUE)
coef <- copy_to(sc, as.data.frame(matrix(0, nrow = bootstraps, ncol = features)), overwrite = TRUE)
# A function to get the number of partitions, which is 1.
# >>>>> R STUFF <<<<<
setwd("~/Dropbox/KSULasso/R/SparkSandbox/")
x = read.csv("x", row.names = 1)
y = read.csv("y", row.names = 1)
colnames(y) <- "dependent"
all_data_R <- cbind(y, x)
alpha = 1
verbose = TRUE
features <- ncol(x)
samples <- nrow(x)
bootstraps <- round(features / samples) * 40
# >>>>> Load R DATA INTO SPARK  <<<<<
# Converts R dataframe into spark dataframe.
all_data <- copy_to(sc, all_data_R, overwrite = TRUE)
coef <- copy_to(sc, as.data.frame(matrix(0, nrow = bootstraps, ncol = features)), overwrite = TRUE)
# A function to get the number of partitions, which is 1.
sdf_num_partitions(all)
sdf_num_partitions(all_data)
test <- all_data %>%
select(c(1, sample(features, samples, replace = FALSE))) %>%
sdf_sample(replacement = TRUE) %>%
ml_linear_regression(response = dependent ~ .,
fit_intercept = TRUE,
lastic_net_param = 1)
attributes(test)
head(test)
attributes(test[7]$coefficients)
test[7]$coefficients[[4]]
test <- all_data %>%
select(c(1, sample(features, samples, replace = FALSE))) %>%
sdf_sample(replacement = TRUE) %>%
ml_linear_regression(response = dependent ~ .,
fit_intercept = TRUE,
lastic_net_param = 1)
attributes(test)
head(test)
attributes(test[1]$coefficients)
attributes(test[7]$coefficients)
